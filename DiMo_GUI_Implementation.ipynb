{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##DiMo-GUI: A Python Implementation of a Visual Grounding Framework\n",
        "\n",
        "This notebook is an implementation of the research paper \"DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning\" ([arXiv:2507.00008v2](https://arxiv.org/pdf/2507.00008v2)). The goal is to replicate the paper's core logic (dynamic zooming and modality decoupling), and test its effectiveness on a general-purpose Vision-Language Model."
      ],
      "metadata": {
        "id": "mjVqlL7m2m7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1NZO_Na62gVc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch pillow accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setup and Helper Functions**\n",
        "\n",
        "Here, we install the necessary Python libraries and load a pre-trained Vision-Language Model (Llava-1.5-7b) from Hugging Face. We use a 4-bit quantized version to ensure it runs within the free Google Colab environment.\n",
        "\n",
        "A key utility function, extract_bbox, is also defined. Since the VLM outputs coordinates in natural language (e.g., \"The bounding box is [128, 352, 307, 417]\"), this function's job is to parse that text. It's designed to handle both absolute pixel values and normalized decimal coordinates, converting them into a standardized list of integers that the Pillow library can use for image manipulation."
      ],
      "metadata": {
        "id": "LTUNcOyWB9a3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq-h82oxuTk"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "# Loaded the model and processor from Hugging Face\n",
        "# Used a 4-bit quantized version to fit in Colab's free GPU memory\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "print(\"Setup Complete! Model is ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Implementing the DiMo-GUI Framework**\n",
        "\n",
        "The core logic of the paper is implemented in three main functions, each corresponding to a stage in the DiMo-GUI process:\n",
        "\n",
        "divide_modalities: This function begins the process by prompting the VLM twice—once for text-only elements and once for icon-only elements—to get two independent initial guesses.\n",
        "\n",
        "dynamic_grounding: This is the iterative refinement stage. It takes an initial guess, crops the image to that region, and asks the model to look again, progressively zooming in to find a more precise location.\n",
        "\n",
        "select_answer: In the final stage, the two refined candidates (one text, one icon) are presented back to the model, which makes a final decision on which one best matches the user's command."
      ],
      "metadata": {
        "id": "uUgLAueVDGqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_bbox(model_output_text, image_width, image_height):\n",
        "    # Updated regex to find numbers that can be integers or decimals\n",
        "    match = re.search(r'\\[(\\s*[\\d\\.]+\\s*,\\s*[\\d\\.]+\\s*,\\s*[\\d\\.]+\\s*,\\s*[\\d\\.]+\\s*)\\]', model_output_text)\n",
        "\n",
        "    if match:\n",
        "        box_str = match.group(1)\n",
        "        try:\n",
        "            # Convert the matched strings to floats first\n",
        "            coords = [float(n.strip()) for n in box_str.split(',')]\n",
        "\n",
        "            # Check if coordinates are normalized (between 0 and 1)\n",
        "            if all(0 <= c <= 1 for c in coords):\n",
        "                # If so, convert them to absolute pixel values\n",
        "                x1 = int(coords[0] * image_width)\n",
        "                y1 = int(coords[1] * image_height)\n",
        "                x2 = int(coords[2] * image_width)\n",
        "                y2 = int(coords[3] * image_height)\n",
        "                return [x1, y1, x2, y2]\n",
        "            else:\n",
        "                # If they are already pixel values, just convert them to integers\n",
        "                return [int(c) for c in coords]\n",
        "\n",
        "        except (ValueError, IndexError):\n",
        "            return None # Handle cases where conversion or parsing fails\n",
        "\n",
        "    return None # Return None if no bounding box is found"
      ],
      "metadata": {
        "id": "rBZiEx6JIeVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_modalities(image, command, model, processor):\n",
        "    width, height = image.size\n",
        "    # Simpler text prompt\n",
        "    text_prompt = f\"USER: <image>\\nFind the TEXT element for the command: '{command}'. Respond with only the bounding box in [x1, y1, x2, y2] format.\\nASSISTANT:\"\n",
        "    inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "    generate_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "    text_output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    initial_text_bbox = extract_bbox(text_output, width, height)\n",
        "    print(f\"Initial Text Guess Output: {text_output}\")\n",
        "\n",
        "    # Simpler icon prompt\n",
        "    icon_prompt = f\"USER: <image>\\nFind the ICON element for the command: '{command}'. Respond with only the bounding box in [x1, y1, x2, y2] format.\\nASSISTANT:\"\n",
        "    inputs = processor(text=icon_prompt, images=image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "    generate_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "    icon_output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    initial_icon_bbox = extract_bbox(icon_output, width, height)\n",
        "    print(f\"Initial Icon Guess Output: {icon_output}\")\n",
        "\n",
        "    return initial_text_bbox, initial_icon_bbox"
      ],
      "metadata": {
        "id": "oxJw3nc5If36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_grounding(image, command, initial_bbox, modality, model, processor, max_iterations=3):\n",
        "    if not initial_bbox:\n",
        "        print(f\"Skipping dynamic grounding for {modality} due to no initial bbox.\")\n",
        "        return None\n",
        "    current_bbox = initial_bbox\n",
        "\n",
        "    # Select the correct simplified prompt\n",
        "    if modality == \"text\":\n",
        "        prompt_template = \"USER: <image>\\nFind the TEXT element for the command: '{command}'. Respond with only the bounding box in [x1, y1, x2, y2] format.\\nASSISTANT:\"\n",
        "    else:\n",
        "        prompt_template = \"USER: <image>\\nFind the ICON element for the command: '{command}'. Respond with only the bounding box in [x1, y1, x2, y2] format.\\nASSISTANT:\"\n",
        "\n",
        "    print(f\"\\n--- Refining {modality.upper()} Bounding Box ---\")\n",
        "    for i in range(max_iterations):\n",
        "        print(f\"Iteration {i+1} with bbox: {current_bbox}\")\n",
        "        box_width = current_bbox[2] - current_bbox[0]\n",
        "        box_height = current_bbox[3] - current_bbox[1]\n",
        "        if box_width < 10 or box_height < 10:\n",
        "            print(f\"  > Bbox size ({box_width}x{box_height}) is too small. Stopping refinement.\")\n",
        "            break\n",
        "\n",
        "        cropped_image = image.crop(current_bbox)\n",
        "        prompt = prompt_template.format(command=command)\n",
        "        inputs = processor(text=prompt, images=cropped_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "        generate_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "        output_text = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "        width, height = cropped_image.size\n",
        "        relative_bbox = extract_bbox(output_text, width, height)\n",
        "        print(f\"  > Model output: {output_text.strip()}\")\n",
        "\n",
        "        if relative_bbox:\n",
        "            origin_x, origin_y = current_bbox[0], current_bbox[1]\n",
        "            current_bbox = [origin_x + relative_bbox[0], origin_y + relative_bbox[1], origin_x + relative_bbox[2], origin_y + relative_bbox[3]]\n",
        "        else:\n",
        "            print(\"  > Could not find a new bbox. Stopping refinement.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Final Refined {modality.upper()} Bbox: {current_bbox}\")\n",
        "    return current_bbox"
      ],
      "metadata": {
        "id": "kfOxeN80IlKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_answer(image, command, final_text_bbox, final_icon_bbox, model, processor):\n",
        "    # Handle cases where one of the candidates might be missing\n",
        "    if not final_text_bbox or not final_icon_bbox:\n",
        "        # If one bbox is missing, default to the one that exists\n",
        "        return final_text_bbox if final_text_bbox else final_icon_bbox\n",
        "\n",
        "    print(\"\\n--- Stage 3: Selecting Final Answer ---\")\n",
        "\n",
        "    # Create the prompt for the final decision, modeled on Figure 3\n",
        "    selection_prompt = (\n",
        "        f\"USER: <image>\\nYou are given a UI screenshot and a user command: '{command}'. \"\n",
        "        f\"There are two candidate UI elements:\\n\"\n",
        "        f\"Candidate 1 (text-based): {final_text_bbox}\\n\"\n",
        "        f\"Candidate 2 (icon-based): {final_icon_bbox}\\n\"\n",
        "        f\"Based on the command and the description of candidates, \"\n",
        "        f\"choose which candidate (1 for text, 2 for icon) better matches the command. \"\n",
        "        f\"Just answer with '1' or '2'.\\nASSISTANT:\"\n",
        "    )\n",
        "\n",
        "    inputs = processor(text=selection_prompt, images=image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "    generate_ids = model.generate(**inputs, max_new_tokens=10) # Only need a short answer\n",
        "    output_text = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    print(f\"Selection Model Output: {output_text.strip()}\")\n",
        "\n",
        "    # Find the final choice in the model's response\n",
        "    if '1' in output_text:\n",
        "        print(\"Model chose Candidate 1 (Text-based).\")\n",
        "        return final_text_bbox\n",
        "    elif '2' in output_text:\n",
        "        print(\"Model chose Candidate 2 (Icon-based).\")\n",
        "        return final_icon_bbox\n",
        "    else:\n",
        "        # Default to one of the boxes if the model gives an unclear answer\n",
        "        print(\"Could not determine choice, defaulting to text-based candidate.\")\n",
        "        return final_text_bbox"
      ],
      "metadata": {
        "id": "mCJvQXISI7Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: The Comparative Experiment**\n",
        "\n",
        "This final section brings everything together to demonstrate the framework's impact. A run_truly_vanilla_test function is defined to establish a baseline by sending a single, generic prompt to the model. This is compared against a run_with_dimo_gui function, which executes the full three-stage process.\n",
        "\n",
        "By running both tests on a small, curated set of challenging UI screenshots, we can visually compare the model's raw performance (the \"vanilla\" result) against its performance when guided by the DiMo-GUI framework. This side-by-side comparison clearly showcases the value and logic of the paper's approach."
      ],
      "metadata": {
        "id": "q-_MGRlUDLrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_vanilla_test(image, command, model, processor):\n",
        "    print(\"\\n--- Running VANILLA Test (Single Generic Prompt) ---\")\n",
        "\n",
        "    image_true_vanilla = image.copy()\n",
        "    draw = ImageDraw.Draw(image_true_vanilla)\n",
        "    width, height = image.size\n",
        "\n",
        "    # New, simpler prompt\n",
        "    generic_prompt = f\"USER: <image>\\nYour task is to find a UI element. What is the bounding box of the element for the command: '{command}'? Respond with only the bounding box in [x1, y1, x2, y2] format.\\nASSISTANT:\"\n",
        "\n",
        "    inputs = processor(text=generic_prompt, images=image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "    generate_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "    output_text = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    bbox = extract_bbox(output_text, width, height)\n",
        "\n",
        "    if bbox:\n",
        "        print(f\"Vanilla Guess: {bbox}\")\n",
        "        draw.rectangle(bbox, outline=\"purple\", width=5)\n",
        "    else:\n",
        "        print(f\"Vanilla model could not find a bounding box. Output: {output_text}\")\n",
        "\n",
        "    return image_true_vanilla"
      ],
      "metadata": {
        "id": "FLSY-SWgTXi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_with_dimo_gui(image, command, model, processor):\n",
        "    print(\"\\n--- Running WITH DiMo-GUI (Full Workflow) ---\")\n",
        "\n",
        "    # Create a copy of the image to draw on\n",
        "    image_dimo = image.copy()\n",
        "    draw = ImageDraw.Draw(image_dimo)\n",
        "\n",
        "    # Stage 1: Divide Modalities\n",
        "    initial_text_bbox, initial_icon_bbox = divide_modalities(image, command, model, processor)\n",
        "\n",
        "    # Stage 2: Dynamic Grounding\n",
        "    final_text_bbox = dynamic_grounding(image, command, initial_text_bbox, \"text\", model, processor)\n",
        "    final_icon_bbox = dynamic_grounding(image, command, initial_icon_bbox, \"icon\", model, processor)\n",
        "\n",
        "    # Stage 3: Select Answer\n",
        "    winning_bbox = select_answer(image, command, final_text_bbox, final_icon_bbox, model, processor)\n",
        "\n",
        "    # Draw the final winning box if it was found\n",
        "    if winning_bbox:\n",
        "        print(f\"\\nDiMo-GUI Winning Bbox: {winning_bbox}\")\n",
        "        draw.rectangle(winning_bbox, outline=\"red\", width=5)\n",
        "    else:\n",
        "        print(\"\\nDiMo-GUI could not determine the final bounding box.\")\n",
        "\n",
        "    return image_dimo"
      ],
      "metadata": {
        "id": "kR6IywG9TY3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our mini-dataset of challenging test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"PowerPoint - Simple Case\",\n",
        "        \"command\": \"Slide show from beginning\",\n",
        "        \"image_path\": \"None\" # The file you uploaded\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Google Maps - Ambiguous Icon/Text\",\n",
        "        \"command\": \"Start Navigation\",\n",
        "        \"image_path\": \"None\" # This should exist from the last run\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    maps_url = \"https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/2147484362/images/78372fc-17c2-bb2-33f0-32d5f745d72_Step_2.PNG\" # Stable URL for a Google Maps screenshot\n",
        "    maps_image = Image.open(requests.get(maps_url, stream=True).raw).convert(\"RGB\")\n",
        "    maps_image.save(\"pres.png\")\n",
        "    test_cases[0][\"image_path\"] = \"pres.png\"\n",
        "except Exception as e:\n",
        "    print(f\"Could not download the first test image: {e}\")\n",
        "    test_cases.pop(1) # Remove the test case if download fails\n",
        "\n",
        "try:\n",
        "    maps_url = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjneJLERTIFbAKsOCK8TSPFGFC3a4eIa3A1fZhkWX1gVXrOU8UcSaq4n95F8iZE8AZELfQK5Y-x-OYkshEKTDJBNc_uEDkxO47js0bykp25iRGPHqs2hcjZ6FGMxpS0W9Qafyd85w5hfGY/s1742/Google-Maps-new-route-UI.jpg\" # Stable URL for a Google Maps screenshot\n",
        "    maps_image = Image.open(requests.get(maps_url, stream=True).raw).convert(\"RGB\")\n",
        "    maps_image.save(\"maps.png\")\n",
        "    test_cases[1][\"image_path\"] = \"maps.png\"\n",
        "except Exception as e:\n",
        "    print(f\"Could not download the second test image: {e}\")\n",
        "    test_cases.pop(1) # Remove the test case if download fails\n",
        "\n",
        "\n",
        "# Loop through each test case and run the new, better comparison\n",
        "for case in test_cases:\n",
        "    print(f\"\\n{'='*20}\\nRunning Test Case: {case['name']}\\nCommand: '{case['command']}'\\n{'='*20}\")\n",
        "\n",
        "    try:\n",
        "        image = Image.open(case[\"image_path\"]).convert(\"RGB\")\n",
        "\n",
        "        # Run the 'Vanilla' test\n",
        "        vanilla_result_image = run_vanilla_test(image, case['command'], model, processor)\n",
        "        print(\"\\nResult VANILLA (Purple=Single Generic Guess):\")\n",
        "        display(vanilla_result_image)\n",
        "\n",
        "        # Run the 'With DiMo-GUI' test\n",
        "        dimo_gui_result_image = run_with_dimo_gui(image, case['command'], model, processor)\n",
        "        print(\"\\nResult WITH DiMo-GUI (Red=Final Answer):\")\n",
        "        display(dimo_gui_result_image)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Could not find the image file '{case['image_path']}'. Please make sure it is uploaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during the test case: {e}\")"
      ],
      "metadata": {
        "id": "A1Bcl8IKThOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Analysis of Results**\n",
        "\n",
        "The experiment shows that the \"Truly Vanilla\" model often fails to locate the correct UI element, and its guesses can be random.\n",
        "\n",
        "While the full DiMo-GUI framework correctly follows its logic (zooming and selecting), its final answer is also incorrect. This is not a bug in the implementation but a demonstration of the model capability gap discussed in the paper. The general-purpose Llava model lacks the specialized training to accurately understand GUIs, causing it to make poor initial guesses that the framework cannot recover from. This highlights the importance of pairing the DiMo-GUI framework with a specialized, GUI-aware model as the authors did."
      ],
      "metadata": {
        "id": "iLwREnbuCVyy"
      }
    }
  ]
}